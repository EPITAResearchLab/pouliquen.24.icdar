{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from torch import nn\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericBackbone(nn.Module):\n",
    "    def __init__(self, backbone_name, pretrained, model_path=\"\", num_classes=-1):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=(pretrained and len(model_path) == 0), num_classes=num_classes)\n",
    "\n",
    "        if model_path is not None and len(model_path) != 0:\n",
    "            print(f\"loading checkpoint from path {model_path}\")\n",
    "            self.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_model = GenericBackbone(\"mobilevit_xxs\", True)\n",
    "imagenet_model = imagenet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the mobilevit_xxs \n",
    "trained on the k0 split using weakly supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please add the path of the model\n",
    "# It can be found in the Parameters of a run in mlflow model.model.model_path\n",
    "# you can create a model with : \"python train.py --config-name=wsl -m +experiment=wsl/mobilevit_s\"\n",
    "# in our case we took the run named \"allvideo_wsl_mobilevit_xxs_k0randomcrop_blur_colorjitter_adamw\" in test_k0\n",
    "wsl_path = \"mlruns/xp/run_name/checkpoints/backbone_path.pth\"\n",
    "wsl_model = GenericBackbone(\"mobilevit_xxs\", False, model_path=wsl_path)\n",
    "wsl_model = wsl_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # obtain the embeddings\n",
    "        embeddings = self.model(input_data).flatten(1)\n",
    "\n",
    "        # Calculate an objective/loss using the embeddings\n",
    "        objective = torch.mean(embeddings, 1)\n",
    "        return objective\n",
    "\n",
    "# Creating feature ablation method (Integrated Gradients) using the model wrapper\n",
    "model_wrapper_wsl = ModelWrapper(wsl_model)\n",
    "model_wrapper_wsl.eval()\n",
    "feature_ablation_wsl = IntegratedGradients(model_wrapper_wsl)\n",
    "\n",
    "model_wrapper_imagenet = ModelWrapper(imagenet_model)\n",
    "model_wrapper_imagenet.eval()\n",
    "feature_ablation_imagenet = IntegratedGradients(model_wrapper_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "#  transforms.Resize((256, 256)),\n",
    " transforms.Resize((224, 224)),\n",
    "#  transforms.CenterCrop(224),\n",
    " transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_normalize = transforms.Normalize(\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "def get_attribution(path, feature_ablation, n_steps=200):\n",
    "\n",
    "    img = Image.open(path)\n",
    "\n",
    "    transformed_img = transform(img)\n",
    "\n",
    "    input = transform_normalize(transformed_img)\n",
    "    input = input.unsqueeze(0)\n",
    "\n",
    "    # Perform feature ablation without specifying a target\n",
    "    attributions = feature_ablation.attribute(input, n_steps=n_steps)\n",
    "    return attributions, transformed_img\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=224)\n",
    "\n",
    "def visu_attr(path, feature_ablation, n_steps=20, cmap=False, use_pyplot=True):\n",
    "    attributions, transformed_img = get_attribution(path, feature_ablation, n_steps)\n",
    "\n",
    "    return viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        [\"original_image\", \"heat_map\"],\n",
    "                                        [\"all\", \"positive\"],\n",
    "                                        cmap=default_cmap if cmap else None,\n",
    "                                        show_colorbar=True,\n",
    "                                        use_pyplot=use_pyplot)\n",
    "# import ImageOps\n",
    "def get_attribution_mirror(path, feature_ablation, n_steps=200):\n",
    "\n",
    "    img = Image.open(path)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    transformed_img = transform(img)\n",
    "\n",
    "    input = transform_normalize(transformed_img)\n",
    "    input = input.unsqueeze(0)\n",
    "\n",
    "    # Perform feature ablation without specifying a target\n",
    "    attributions = feature_ablation.attribute(input, n_steps=n_steps)\n",
    "    return attributions, transformed_img\n",
    "\n",
    "\n",
    "def visu_attr_mirror(path, feature_ablation, n_steps=20, cmap=False):\n",
    "    attributions, transformed_img = get_attribution_mirror(path, feature_ablation, n_steps)\n",
    "\n",
    "    _ = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        [\"original_image\", \"heat_map\"],\n",
    "                                        [\"all\", \"positive\"],\n",
    "                                        cmap=default_cmap if cmap else None,\n",
    "                                        show_colorbar=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_attr_wsl(path, n_steps=20, cmap=False, use_pyplot=True):\n",
    "    return visu_attr(path, feature_ablation_wsl, n_steps, cmap, use_pyplot)\n",
    "\n",
    "def visu_attr_imagenet(path, n_steps=20, cmap=False, use_pyplot=True):\n",
    "    return visu_attr(path, feature_ablation_imagenet, n_steps, cmap, use_pyplot=use_pyplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## midv holo fake\n",
    "with glare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before training\")\n",
    "visu_attr_imagenet(\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0042.jpg\", 20)\n",
    "visu_attr_imagenet(\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0021.jpg\", 20)\n",
    "visu_attr_imagenet(\"../../data/midv-holo/crop_ovds/fraud/copy_without_holo/ID/id06_05_01/img_0021.jpg\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"after training\")\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0042.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0021.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/fraud/copy_without_holo/ID/id06_05_01/img_0021.jpg\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wsl\")\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_02/img_0038.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_02/img_0033.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_03/img_0037.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_03/img_0011.jpg\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_export = [\n",
    "\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_02/img_0038.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_02/img_0033.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_03/img_0037.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/origins/ID/id10_03_03/img_0011.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0042.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id06_05_01/img_0021.jpg\",\n",
    "\"../../data/midv-holo/crop_ovds/fraud/copy_without_holo/ID/id06_05_01/img_0021.jpg\"]\n",
    "for p in to_export:\n",
    "    f, _ = visu_attr_wsl(p, 20, use_pyplot=False)\n",
    "    f.savefig(f\"samples/train/figure/wsl/wsl_{os.path.basename(p)}\")\n",
    "    f, _ = visu_attr_imagenet(p, 20, use_pyplot=False)\n",
    "    f.savefig(f\"samples/train/figure/imagenet/imagenet_{os.path.basename(p)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random selection of Origins sample in train and test set\n",
    "some images are available in `notebooks/visualisation/samples/train` and `notebooks/visualisation/samples/test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/splits_kfold_s0/k0/fraud/copy_without_holo/trainval/train_train.txt\") as f:\n",
    "    train_video = f.read().splitlines(False)\n",
    "\n",
    "with open(\"../../data/splits_kfold_s0/k0/origins/test.txt\") as f:\n",
    "    test_video = f.read().splitlines(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "frames = []\n",
    "for v in random.sample(train_video, 10):\n",
    "    p_v = os.path.dirname(v)\n",
    "    image_p = os.path.join(\"../../data/midv-holo/crop_ovds/origins\", p_v)\n",
    "    p_glob = os.path.join(image_p, \"*.jpg\")\n",
    "    fs = glob.glob(p_glob)\n",
    "    for f in random.sample(fs, 1):\n",
    "        frames.append(os.path.join(image_p, f))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visu_attr_wsl(frames[i], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    f, _ = visu_attr_wsl(frames[i], 20, use_pyplot=False)\n",
    "    f.savefig(f\"samples/train/wsl_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_test = []\n",
    "for v in random.sample(train_video, 10):\n",
    "    p_v = os.path.dirname(v)\n",
    "    image_p = os.path.join(\"../../data/midv-holo/crop_ovds/origins\", p_v)\n",
    "    p_glob = os.path.join(image_p, \"*.jpg\")\n",
    "    fs = glob.glob(p_glob)\n",
    "    for f in random.sample(fs, 1):\n",
    "        frames_test.append(os.path.join(image_p, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visu_attr_wsl(frames_test[i], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    f, _ = visu_attr_wsl(frames_test[i], 20, use_pyplot=False)\n",
    "    f.savefig(f\"samples/test/wsl_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDV 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_attr_wsl(\"../../data/midv-2020/clips/crop_ovd/alb_id/01/000001.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-2020/clips/crop_ovd/aze_passport/01/000001.jpg\", 20)\n",
    "visu_attr_wsl(\"../../data/midv-2020/clips/crop_ovd/fin_id/01/000001.jpg\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video of attribution maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir activation_map\n",
    "%mkdir activation_map_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "n_steps = 20\n",
    "base_path = \"../../data/midv-holo/crop_ovds/origins/ID/id03_04_02/\"\n",
    "paths = glob.glob(f\"{base_path}*.jpg\")\n",
    "# print(paths)\n",
    "paths = list(sorted(paths, key=lambda p: int(p[p.rfind(\"img_\")+5:p.rfind(\".\")])))\n",
    "\n",
    "for i, path in tqdm(enumerate(paths), total=len(paths)):\n",
    "    attributions, transformed_img = get_attribution(path, feature_ablation_wsl, n_steps)\n",
    "\n",
    "    fig, a = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                            np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                            [\"original_image\", \"heat_map\"],\n",
    "                                            [\"all\", \"positive\"],\n",
    "                                            # cmap=default_cmap,\n",
    "                                            show_colorbar=True, use_pyplot=False)\n",
    "    fig.savefig(f\"activation_map/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../data/midv-holo/crop_ovds/fraud/photo_holo_copy/ID/id03_04_01/\"\n",
    "paths = glob.glob(f\"{base_path}*.jpg\")\n",
    "paths = list(sorted(paths, key=lambda p: int(p[p.rfind(\"img_\")+5:p.rfind(\".\")])))\n",
    "os\n",
    "for i, path in tqdm(enumerate(paths), total=len(paths)):\n",
    "    attributions, transformed_img = get_attribution(path, feature_ablation_wsl, n_steps)\n",
    "\n",
    "    fig, a = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                            np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                            [\"original_image\", \"heat_map\"],\n",
    "                                            [\"all\", \"positive\"],\n",
    "                                            # cmap=default_cmap,\n",
    "                                            show_colorbar=True, use_pyplot=False)\n",
    "    fig.savefig(f\"activation_map_fake/{i}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
