{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_from_mlflow = False\n",
    "save_results_file = \"reproduction_midvholo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrieve_from_mlflow:\n",
    "    print(\"retreiving results from mlflow\")\n",
    "    mlflow.set_tracking_uri(\"../mlruns\") # local\n",
    "\n",
    "    client = mlflow.MlflowClient()\n",
    "\n",
    "    experiment_name = f\"cumulative_midv_baseline_fulldoc_nosplit\"\n",
    "    current_experiment = dict(client.get_experiment_by_name(experiment_name))\n",
    "    print(\"found\", current_experiment[\"name\"])\n",
    "    experiment = current_experiment['experiment_id']\n",
    "    runs = mlflow.search_runs([experiment], filter_string=\"attributes.status = 'FINISHED'\", order_by=[\"start_time DESC\"])\n",
    "    runs_tmp = runs[[\"params.model\", \"params.decision\"]]\n",
    "    runs = runs.filter(regex=\"^metrics\")\n",
    "    runs = pd.concat((runs, runs_tmp), axis=1)\n",
    "\n",
    "    for i, row in runs.iterrows(): # extract the params :s_t, T, h_t (named th)\n",
    "        for k in [\"params.model\", \"params.decision\"]:\n",
    "            params_model_dict = ast.literal_eval(row[k])\n",
    "            for k, v in params_model_dict.items():\n",
    "                if k != \"_target_\":\n",
    "                    if k not in runs.columns:\n",
    "                        runs[k] = None  # Create a new column if it doesn't exist\n",
    "                    runs.at[i, k] = v\n",
    "    print(f\"writing {save_results_file}\")\n",
    "    runs.to_csv(save_results_file)\n",
    "else:\n",
    "    print(\"retrieving results from\", save_results_file)\n",
    "    runs = pd.read_csv(save_results_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[\"metrics.fpr\"] = 1-runs[\"metrics.specificity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.loc[\n",
    "    (runs[\"s_t\"] == 50) & (runs[\"hight_threshold\"] == 240) & (runs[\"th\"] == 0.01)\n",
    "].sort_values(\"T\")[[\"metrics.recall\", \"metrics.fpr\", \"s_t\", \"T\", \"hight_threshold\", \"th\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_t = 50\n",
    "th = 0.01 # h_t\n",
    "runs_filtered = runs.loc[\n",
    "    (runs[\"s_t\"] == s_t) & (runs[\"hight_threshold\"] == 240) & (runs[\"th\"] == th)\n",
    "]\n",
    "runs_filtered = runs_filtered.sort_values(\"T\", ascending=True)\n",
    "x = list(runs_filtered[\"metrics.fpr\"])\n",
    "y = list(runs_filtered[\"metrics.recall\"]) \n",
    "\n",
    "# This is the ROC curve\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.step(x,y, where=\"pre\")\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.xlabel(\"False positive rate (FPR)\")\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "ticks = np.arange(0, 100, 10)/100\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.grid(which=\"both\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(f\"ROC curve for s_t={s_t} and h_t={th}\")\n",
    "plt.legend([f\"AUC {auc(x,y).round(3)}\", f\"Random {auc([0,1],[0,1]).round(3)}\"], loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_group(df):\n",
    "    df = df.sort_values(\"T\", ascending=True)\n",
    "    x = list(df[\"metrics.fpr\"])\n",
    "    y = list(df[\"metrics.recall\"]) \n",
    "    return auc(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_serie = runs.groupby([\"s_t\",\"th\"]).apply(auc_group)\n",
    "auc_df = pd.DataFrame(auc_serie)\n",
    "auc_df = auc_df.T.style.format(precision=3)\n",
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the previous cell:\n",
    "```latex\n",
    "\\begin{tabular}{lrrrrrrrrr}\n",
    "s_t & \\multicolumn{3}{r}{30} & \\multicolumn{3}{r}{40} & \\multicolumn{3}{r}{50} \\\\\n",
    "th & 0.010000 & 0.020000 & 0.030000 & 0.010000 & 0.020000 & 0.030000 & 0.010000 & 0.020000 & 0.030000 \\\\\n",
    "hight_threshold & 240 & 240 & 240 & 240 & 240 & 240 & 240 & 240 & 240 \\\\\n",
    "0 & 0.838 & 0.846 & 0.844 & 0.855 & 0.844 & 0.831 & 0.857 & 0.826 & 0.790 \\\\\n",
    "\\end{tabular}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
